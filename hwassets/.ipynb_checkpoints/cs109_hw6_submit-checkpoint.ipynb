{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 6\n",
    "# Reg-Logistic Regression, ROC, and Data Imputation\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately):\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A): 209A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Breast Cancer Detection\n",
    "\n",
    "In this homework, we will consider the problem of early breast cancer detection from X-ray images. Specifically, given a candidate region of interest (ROI) from an X-ray image of a patient's breast, the goal is to predict if the region corresponds to a malignant tumor (label 1) or is normal (label 0). The training and test data sets for this problem is provided in the file `hw6_dataset.csv`. Each row in these files corresponds to a ROI in a patient's X-ray, with columns 1-117 containing features computed using standard image processing algorithms. The last column contains the class label, and is based on a radiologist's opinion or a biopsy. This data was obtained from the KDD Cup 2008 challenge.\n",
    "\n",
    "The data set contain a total of 69,098 candidate ROIs, of which only 409 are malignant, while the remaining are all normal. \n",
    "\n",
    "*Note*: be careful of reading/treating column names and row names in this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Beyond Classification Accuracy\n",
    "\n",
    "\n",
    "0.  Split the data set into a training set and a testing set.  The training set should be 75% of the original data set, and the testing set 25%.  Use `np.random.seed(9001)`.\n",
    "\n",
    "1. Fit a logistic regression classifier to the training set and report the  accuracy of the classifier on the test set. You should use $L_2$ regularization in logistic regression, with the regularization parameter tuned using cross-validation. \n",
    "    1. How does the fitted model compare with a classifier that predicts 'normal' (label 0) on all patients? \n",
    "    2. Do you think the difference in the classification accuracies are large enough to declare logistic regression as a better classifier than the all 0's classifier? Why or why not?\n",
    "    \n",
    "For applications with imbalanced class labels, in this case when there are many more healthy subjects ($Y=0$) than those with cancer ($Y=1$), the classification accuracy may not be the best metric to evaluate a classifier's performance. As an alternative, we could analyze the confusion table for the classifier. \n",
    "\n",
    "<ol start=\"3\">\n",
    "<li> Compute the confusion table for both the fitted classifier and the classifier that predicts all 0's.</li>\n",
    "<li> Using the entries of the confusion table compute the *true positive rate* and the *true negative rate* for the two classifiers. Explain what these evaluation metrics mean for the specific task of cancer detection. Based on the observed metrics, comment on whether the fitted model is better than the all 0's classifier.</li>\n",
    "<li> What is the *false positive rate* of the fitted classifier, and how is it related to its true positive and true negative rate? Why is a classifier with high false positive rate undesirable for a cancer detection task?</li>\n",
    "</ol>\n",
    "*Hint:* You may use the `metrics.confusion_matrix` function to compute the confusion matrix for a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "np.random.seed(9001)\n",
    "df = pd.read_csv('data/hw6_dataset.csv', header=None)\n",
    "msk = np.random.rand(len(df)) < 0.75\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52009, 118)\n",
      "(17089, 118)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.144  , -0.143  , -0.116  , ...,  0.553  , -0.417  ,  0.256  ],\n",
       "       [ 0.215  , -0.184  ,  0.0274 , ...,  0.363  ,  0.326  , -0.0528 ],\n",
       "       [ 0.00922, -0.138  ,  0.169  , ...,  0.55   , -0.284  ,  0.155  ],\n",
       "       ..., \n",
       "       [ 0.136  ,  0.186  ,  0.0736 , ..., -0.219  ,  0.549  , -0.373  ],\n",
       "       [ 0.0559 ,  0.201  ,  0.212  , ...,  0.474  ,  1.1    , -0.212  ],\n",
       "       [-0.0988 ,  0.185  , -0.317  , ...,  0.0308 ,  0.456  , -0.394  ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8184761c94fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mlogitm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_normed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                       \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                       )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_encoded_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m             for train, test in folds)\n\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_log_reg_scoring_path\u001b[0;34m(X, y, train, test, pos_class, Cs, scoring, fit_intercept, max_iter, tol, class_weight, verbose, solver, penalty, dual, intercept_scaling, multi_class, random_state, max_squared_sum, sample_weight)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mlog_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mlogistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, copy, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m                     iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)\n\u001b[0m\u001b[1;32m    710\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m                 \u001b[0;31m# old scipy doesn't have maxiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 193\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    194\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    195\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_logistic_loss_and_grad\u001b[0;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_intercept_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_intercept_dot\u001b[0;34m(w, X, y)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0myz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train = data_train.iloc[:, :-1].values\n",
    "y_train = data_train.iloc[:, -1].values\n",
    "X_test = data_test.iloc[:, :-1].values\n",
    "y_test = data_test.iloc[:, -1].values\n",
    "\n",
    "# Standardize\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_normed = scaler.transform(X_train)\n",
    "X_test_normed = scaler.transform(X_test)\n",
    "\n",
    "# Fitting\n",
    "\n",
    "logitm = LogisticRegressionCV(penalty='l2').fit(X_train_normed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the logistic regression classifier on the test set is 0.995026040142782.\n",
      "The accuracy of the classifier predicting 'normal' on all patients on the test set is 0.9942067996957107.\n",
      "The increase in accuracy on the test set (by percentage) is 0.0008240141259564976.\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = logitm.predict(X_test_normed)\n",
    "test_score = accuracy_score(y_test, y_test_pred)\n",
    "print('The accuracy of the logistic regression classifier on the test set is {}.'.format(test_score))\n",
    "print('The accuracy of the classifier predicting \\'normal\\' on all patients on the test set is {}.'.\\\n",
    "     format(np.mean(y_test==0)))\n",
    "print('The increase in accuracy on the test set (by percentage) is {}.'.\\\n",
    "      format((test_score-np.mean(y_test==0))/np.mean(y_test==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. The accuracy of the logistic regression classifier on the test set is slightly better than the classifier predicting 'normal' on all patients (by 0.082%).\n",
    "\n",
    "B. We don't think the difference in the classification accuracies is large enough to declare logistic regression as a better classifier than the all 0's classifier since the difference is too small (only 0.082%). The reason is that non-zero samples are too small. The class labels are imbalanced, which indicates other metrics (such as false positive rate and false negative rate) might provide better evaluation for the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion table for the fitted classifier is [[16984     6]\n",
      " [   79    20]].\n",
      "The confusion table for the classifier that predicts all 0s is [[16990     0]\n",
      " [   99     0]].\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test_pred2 = np.zeros_like(y_test)\n",
    "\n",
    "cm1 = confusion_matrix(y_test, y_test_pred)\n",
    "cm2 = confusion_matrix(y_test, y_test_pred2)\n",
    "print('The confusion table for the fitted classifier is {}.'.format(cm1))\n",
    "print('The confusion table for the classifier that predicts all 0s is {}.'.format(cm2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fitted classifier: true positive rate 0.20202020202020202; true negative rate 0.9996468510888759.\n",
      "The classifier that predicts all 0s: true positive rate 0.0; true negative rate 1.0.\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "\n",
    "def cal_tpr(cm):\n",
    "    return cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "def cal_tnr(cm):\n",
    "    return cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "print('The fitted classifier: true positive rate {}; true negative rate {}.'.format(cal_tpr(cm1), cal_tnr(cm1)))\n",
    "print('The classifier that predicts all 0s: true positive rate {}; true negative rate {}.'.\\\n",
    "      format(cal_tpr(cm2), cal_tnr(cm2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, true positive rate means the percentage of malignant tumors that have been correctly predicted (among all cancer patients); true negative rate means the percentage of normal patients that have been correctly predicted (among all normal patients).\n",
    "\n",
    "Based on the observed metrics, the fitted model is better than the all 0s classifier since the true positive rate is much higher, which means more cancer patients are diagnosed while no cancer patients could be diagnosed through the classifier predicting 0s for all patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fitted classifier: false positive rate 0.0003531489111241907.\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "\n",
    "def cal_fpr(cm):\n",
    "    return cm[0, 1] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "print('The fitted classifier: false positive rate {}.'.format(cal_fpr(cm1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The false positive rate of the fitted classifier is shown above. \n",
    "$$False\\ Positive\\ Rate = \\frac{\\sum False\\ Positive}{\\sum Condition\\ Positive}; \n",
    "True\\ Positive\\ Rate = \\frac{\\sum True\\ Positive}{\\sum Condition\\ Negative},$$\n",
    "which means the relation between false positive rate and true positive rate depends on the number of actual positives and negatives in sample, as well as the number of true positive and false positive.\n",
    "$$False\\ Positive\\ Rate = 1 - True\\ Negative\\ Rate.$$\n",
    "\n",
    "It is also equal to the $$\\frac{total \\ number \\ label \\ as \\ true (1) - true \\ positive}{total \\ number \\ of \\ actually \\ false(0)}$$\n",
    "\n",
    "A classifier with high false positive rate means many healthy people would be misdiagnosed as cancer patients. It is thus undersirable since it might treat the normal patients wrongly, increase the burden of hospital and unnecessary worriness of patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: ROC Analysis\n",
    "\n",
    "Another powerful diagnostic tool for class-imbalanced classification tasks is the Receiver Operating Characteristic (ROC) curve. Notice that the default logistic regression classifier in `sklearn` classifies a data point by thresholding the predicted class probability $\\hat{P}(Y=1)$ at 0.5. By using a different threshold, we can adjust the trade-off between the true positive rate (TPR) and false positive rate (FPR) of the classifier. The ROC curve allows us to visualize this trade-off across all possible thresholds.\n",
    "\n",
    "\n",
    "1. Display the ROC curve for the fitted classifier on the *test set*. In the same plot, also display the ROC curve for the all 0's classifier. How do the two curves compare?\n",
    "\n",
    "2.  Compute the highest TPR that can be achieved by the classifier at each of the following FPR's, and the thresholds at which they are achieved. Based on your results, comment on how the threshold influences a classifier's FPR.\n",
    "    - FPR = 0\n",
    "    - FPR = 0.1\n",
    "    - FPR = 0.5\n",
    "    - FPR = 0.9\n",
    "- Suppose a clinician told you that diagnosing a cancer patient as normal is *twice* as critical an error as diagnosing a normal patient as having cancer. Based on this information, what threshold would you recommend the clinician to use? What is the TPR and FPR of the classifier at this threshold? \n",
    "\n",
    "- Compute the area under the ROC curve (AUC) for both the fitted classifier and the all 0's classifier. How does the difference in the AUCs of the two classifiers compare with the difference between their classification accuracies in Question 1, Part 2(A)? \n",
    "\n",
    "*Hint:* You may use the `metrics.roc_curve` function to compute the ROC curve for a classification model and the `metrics.roc_auc_score` function to compute the AUC for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAHxCAYAAAAStaShAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118b01470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "y_test_proba = logitm.predict_proba(X_test_normed)[:, 1]\n",
    "fpr1, tpr1, thres1 = metrics.roc_curve(y_test, y_test_proba, drop_intermediate=False)\n",
    "\n",
    "\n",
    "y_test_s2 = np.zeros_like(y_test)\n",
    "fpr2, tpr2, thres2 = metrics.roc_curve(y_test, y_test_s2, drop_intermediate=False)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr1, tpr1, label='Logistic regression');\n",
    "plt.scatter(fpr2, tpr2, color='k', label='All 0\\'s classifier');\n",
    "plt.plot([0, 1], [0, 1], '--');\n",
    "plt.xlabel('False positive rate');\n",
    "plt.ylabel('True positive rate');\n",
    "plt.legend();\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the ROC curve for the fitted classifier approaches the upper left corner, which corresponds to the best possible prediction result, at some thresholds, while the ROC curve for the all 0's classifier contains only 2 points , which are (0, 0) and (1, 1). All 0's classifier could flip only by thresholding $\\hat{P}(Y=1)$ at 0, which becomes all 1's classifier and reaches a true positive rate of 1 at the cost of false positive rate of 1; both true positive rate and false positive rate are 0 for any other thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When FPR = 0, the highest TPR is 0.08080808080808081 by thresholding at 0.9403685975225237.\n",
      "When FPR = 0.1, the highest TPR is 0.8383838383838383 by thresholding at 0.0062472431097789.\n",
      "When FPR = 0.5, the highest TPR is 0.9696969696969697 by thresholding at 0.00020945428351840822.\n",
      "When FPR = 0.9, the highest TPR is 1.0 by thresholding at 2.4854580327236744e-06.\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "def best_tpr(fpr, fpr_=fpr1, tpr_=tpr1, thres_=thres1):\n",
    "    i = np.argmax(tpr_[fpr_==fpr])\n",
    "    btpr = tpr_[fpr_==fpr][i]\n",
    "    bthres = thres_[fpr_==fpr][i]\n",
    "    return btpr, bthres\n",
    "\n",
    "fprs = [0, 0.1, 0.5, 0.9]\n",
    "for fpr in fprs:\n",
    "    btpr, bthres = best_tpr(fpr)\n",
    "    print('When FPR = {}, the highest TPR is {} by thresholding at {}.'.format(fpr, btpr, bthres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classifier's FPR increases as the threshold decreases, since each sample is more likely to be labeled as \"positive\" when the threshold decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. If diagnosing a cancer patient as normal is twice as critical an error as diagnosing a normal patient as having cancer, we can define the loss function as $2\\times FNR + FPR = 2\\times (1-TPR) + FPR$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recommended threshold is 0.005752301144082792; the corresponding TPR is 0.8484848484848485; the corresponding FPR is 0.10512065921130076.\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "\n",
    "def loss_function(fpr, tpr):\n",
    "    return 2 * (1 - tpr) + fpr\n",
    "\n",
    "loss = loss_function(fpr1, tpr1)\n",
    "i = np.argmin(loss)\n",
    "print('The recommended threshold is {}; the corresponding TPR is {}; the corresponding FPR is {}.'.\\\n",
    "     format(thres1[i], tpr1[i], fpr1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for the fitted classifier is 0.9346353469955588.\n",
      "The AUC for the all 0's classifier is 0.5.\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc1 = roc_auc_score(y_test, y_test_proba)\n",
    "auc2 = roc_auc_score(y_test, y_test_s2)\n",
    "\n",
    "print('The AUC for the fitted classifier is {}.'.format(auc1))\n",
    "print('The AUC for the all 0\\'s classifier is {}.'.format(auc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The AUC for the fitted classifier is significantly better than that for the all 0's classifier, while there is no significant difference between their classification accuracies in Question 1, Part 2(A). \n",
    "\n",
    "The ROC curve of fitted model looks better. Also, its AUC is close to the accuracy in Question 1. However, AUC of all 0's classifier is much less than accuracy in Question 1. This infers that using ROC/AUC might be a better measure to assess a model rather than simply looking at accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 3: Missing data\n",
    "\n",
    "In this problem you are given a different data set, `hw6_dataset_missing.csv`, that is  similar to the one you used above (same column definitions and same conditions), however this data set contains missing values. \n",
    "\n",
    "*Note*: be careful of reading/treating column names and row names in this data set as well, it *may* be different than the first data set.\n",
    "\n",
    "\n",
    "1. Remove all observations that contain and missing values, split the dataset into a 75-25 train-test split, and fit the regularized logistic regression as in Question 1 (use `LogisticRegressionCV` again to retune).  Report the overall classification rate and TPR in the test set.\n",
    "2. Restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via mean imputation.  Split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
    "3. Again restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via a model-based imputation method. Once again split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
    "4. Compare the results in the 3 previous parts of this problem.  Prepare a paragraph (5-6 sentences) discussing the results, the computational complexity of the methods, and conjecture and explain why you get the results that you see.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24999, 118)\n",
      "(1436, 118)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "\n",
    "df = pd.read_csv('HW6_dataset_missing.csv')\n",
    "df = df.drop('Unnamed: 0', 1)\n",
    "print(df.shape)\n",
    "df = df.dropna(how='any')\n",
    "print(df.shape)\n",
    "print(np.sum(df.iloc[:, -1] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.9943820224719101.\n",
      "The TPR on the test set is 0.0.\n",
      "The AUC on the test set is 0.8008474576271186.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wyssuser/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(9001)\n",
    "msk = np.random.rand(len(df)) < 0.75\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]\n",
    "\n",
    "X_train = data_train.iloc[:, :-1].values\n",
    "y_train = data_train.iloc[:, -1].values\n",
    "X_test = data_test.iloc[:, :-1].values\n",
    "y_test = data_test.iloc[:, -1].values\n",
    "\n",
    "# Standardize\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_normed = scaler.transform(X_train)\n",
    "X_test_normed = scaler.transform(X_test)\n",
    "\n",
    "# Fitting\n",
    "\n",
    "logitm = LogisticRegressionCV(penalty='l2').fit(X_train_normed, y_train)\n",
    "\n",
    "y_test_pred = logitm.predict(X_test_normed)\n",
    "test_score = accuracy_score(y_test, y_test_pred)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "def cal_tpr(cm):\n",
    "    return cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "print('The accuracy on the test set is {}.'.format(test_score))\n",
    "print('The TPR on the test set is {}.'.format(cal_tpr(cm)))\n",
    "\n",
    "y_test_proba = logitm.predict_proba(X_test_normed)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_test_proba)\n",
    "print('The AUC on the test set is {}.'.format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24999, 118)\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "df = pd.read_csv('HW6_dataset_missing.csv')\n",
    "df = df.drop('Unnamed: 0', 1)\n",
    "print(df.shape)\n",
    "\n",
    "# mean imputation\n",
    "df = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.9929795918367347.\n",
      "The TPR on the test set is 0.19607843137254902.\n",
      "The AUC on the test set is 0.8370005229619012.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(9001)\n",
    "msk = np.random.rand(len(df)) < 0.75\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]\n",
    "\n",
    "X_train = data_train.iloc[:, :-1].values\n",
    "y_train = data_train.iloc[:, -1].values\n",
    "X_test = data_test.iloc[:, :-1].values\n",
    "y_test = data_test.iloc[:, -1].values\n",
    "\n",
    "# Standardize\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_normed = scaler.transform(X_train)\n",
    "X_test_normed = scaler.transform(X_test)\n",
    "\n",
    "# Fitting\n",
    "\n",
    "logitm = LogisticRegressionCV(penalty='l2').fit(X_train_normed, y_train)\n",
    "\n",
    "y_test_pred = logitm.predict(X_test_normed)\n",
    "test_score = accuracy_score(y_test, y_test_pred)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "def cal_tpr(cm):\n",
    "    return cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "print('The accuracy on the test set is {}.'.format(test_score))\n",
    "print('The TPR on the test set is {}.'.format(cal_tpr(cm)))\n",
    "\n",
    "y_test_proba = logitm.predict_proba(X_test_normed)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_test_proba)\n",
    "print('The AUC on the test set is {}.'.format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24999, 118)\n",
      "The number of missing values in the class label column is 0.\n",
      "The number of columns without missing values (not including the class label column) is 100.\n",
      "The number of columns with missing values is 17.\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "\n",
    "df = pd.read_csv('HW6_dataset_missing.csv')\n",
    "df = df.drop('Unnamed: 0', 1)\n",
    "print(df.shape)\n",
    "print('The number of missing values in the class label column is {}.'.format(sum(df.iloc[:, -1].isnull())))\n",
    "\n",
    "# Find columns with missing data\n",
    "cols_intact = []\n",
    "cols_missing = []\n",
    "for i in range(df.shape[1] - 1):\n",
    "    if sum(df.iloc[:, i].isnull()) > 0:\n",
    "        cols_missing.append(i)\n",
    "    else:\n",
    "        cols_intact.append(i)\n",
    "\n",
    "print('The number of columns without missing values (not including the class label column) is {}.'.\\\n",
    "      format(len(cols_intact)))\n",
    "print('The number of columns with missing values is {}.'.format(len(cols_missing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most columns don't contain any missing value; we can thus impute missing values by fitting a Ridge regression model on these intact columns and plugging in predicted values plus randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def impute(i, df=df, cols_intact=cols_intact):\n",
    "    X_imp = df.iloc[:, cols_intact]\n",
    "    y_imp = df.iloc[:, i]\n",
    "    X_reg = X_imp[~y_imp.isnull()]\n",
    "    y_reg = y_imp[~y_imp.isnull()]\n",
    "    \n",
    "    regress = RidgeCV().fit(X_reg, y_reg)\n",
    "    y_hat = regress.predict(X_imp)\n",
    "    y_reg_pred = y_hat[~y_imp.isnull()]\n",
    "    \n",
    "    y_missing = y_hat[y_imp.isnull()]\n",
    "    y_missing_noise = y_missing + np.random.normal(loc=0, scale=np.sqrt(mean_squared_error(y_reg, y_reg_pred)), \\\n",
    "                                                   size=y_missing.shape[0])\n",
    "    missing_series = pd.Series(data=y_missing_noise, index=y_imp[y_imp.isnull()].index)\n",
    "    df.iloc[:, i] = df.iloc[:, i].fillna(missing_series)\n",
    "    return df\n",
    "\n",
    "for i in cols_missing:\n",
    "    df = impute(i, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.9929795918367347.\n",
      "The TPR on the test set is 0.17647058823529413.\n",
      "The AUC on the test set is 0.84705301284162.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(9001)\n",
    "msk = np.random.rand(len(df)) < 0.75\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]\n",
    "\n",
    "X_train = data_train.iloc[:, :-1].values\n",
    "y_train = data_train.iloc[:, -1].values\n",
    "X_test = data_test.iloc[:, :-1].values\n",
    "y_test = data_test.iloc[:, -1].values\n",
    "\n",
    "# Standardize\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_normed = scaler.transform(X_train)\n",
    "X_test_normed = scaler.transform(X_test)\n",
    "\n",
    "# Fitting\n",
    "\n",
    "logitm = LogisticRegressionCV(penalty='l2').fit(X_train_normed, y_train)\n",
    "\n",
    "y_test_pred = logitm.predict(X_test_normed)\n",
    "test_score = accuracy_score(y_test, y_test_pred)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "def cal_tpr(cm):\n",
    "    return cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "print('The accuracy on the test set is {}.'.format(test_score))\n",
    "print('The TPR on the test set is {}.'.format(cal_tpr(cm)))\n",
    "\n",
    "y_test_proba = logitm.predict_proba(X_test_normed)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_test_proba)\n",
    "print('The AUC on the test set is {}.'.format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Q4 answer:\n",
    "**Results**: the classification accuracies are very similar for 3 methods; the accuracy of the first method is slightly higher than that of other 2 methods, which is mainly due to smaller sample size. The TPR for the first method is 0, while the TPRs for the other 2 methods are between 0.15 and 0.2 ,where the TPR for the mena imputation method is slightly higher. The AUC for the mean imputation method is higher than that of the first one; the AUC for the model imputation is higher than that of the mean imputation method.\n",
    "\n",
    "**Computational complexity**: let $N$ be the number of samples and $C$ be the number of features. It takes $O(CN)$ to either drop the observations that contain missing values or perform mean imputation; it takes $O(C^2N)$ to impute missing values for a single column through linear regression (the coefficient and thus the time cost here for the 3rd method is higher due to regularization and cross validation cost). The computational costs of logistic regression for the 2nd and 3rd imputation methods are the same, while the computational cost (in terms of both time and space) of logistic regression for the 1st method is much lower since the leftover sample size is significantly lower.\n",
    "\n",
    "**Discussion**: the sample size after removing missing values is significantly lower (by an order of magnitude) and there are only 3 samples with positive labels left in total; as a result, both the classification model and the classification result are very sensitive to the noise in these 3 samples, and the TPR would be 0 if none of these samples is predicted correctly. Therefore, it is not a good idea to remove missing values here. The difference in TPRs for the mean imputation and the model imputation might be caused by the choice of the threshold (we chose the default threshold here) since the AUC score for the model imputation method is higher than that of the mean imputation method while the TPR is lower. Higher AUC score for the model imputation method indicates better performance of the 3rd method. The mean imputation could easily deviate from real data since the same value is used for different observations; the result that it gets better performance than dropping missing values might be simply due to the fact that it keeps all the observations since the features from intact columns, which dominate the feature space, could contribute significantly to the classification. The model imputation method, which used Ridge regression with uncertainty, mimicking real data better than the mean imputation, thus gets the best AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APCOMP209a - Homework Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "This problem walks you through the derivation of the **likelihood equations** for a generalized linear model (GLM). Suppose that the random component of the GLM is in the univariate natural exponential family, so that\n",
    "$$f(y_i|\\theta_i) = h(y_i) e^{y_i\\theta_i - b(\\theta_i)}$$\n",
    "Define the individual log-likelihood for each observation $i$ as\n",
    "$$l_i(\\theta_i) \\equiv \\log f(y_i|\\theta_i)$$\n",
    "with linear predictor\n",
    "$$\\eta_i = x_i^T\\beta = g(\\mu_i)$$\n",
    "for some link function $g$ and where $\\mu_i=E(Y_i)$.\n",
    "\n",
    "1. Use the above expressions to write a simplified expression for the log-likelihood $l(\\theta)$ for the entire dataset, $y_1, \\dots, y_n$.\n",
    "\n",
    "2. Use the chain rule to express $\\frac{\\partial l_i}{\\partial \\beta_j}$ in terms of the derivatives of $l_i, \\theta_i, \\mu_i$, and $\\eta_i$. (*Hint*: Think carefully about which variables are related to which, and in what way. For example, for which of the above variables do you know the derivative with respect to $\\beta_j$?)\n",
    "\n",
    "3. Compute the derivatives for $\\frac{\\partial l_i}{\\partial \\theta_i}$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_j}$.\n",
    "\n",
    "4. Express $\\mu_i$ in terms of $\\theta_i$, and use this relationship to compute $\\frac{\\partial \\theta_i}{\\partial \\mu_i}$. (\\emph{Hint}: Recall the cumulant function of a natural exponential family, and assume that you can write $\\partial f/\\partial g = (\\partial g / \\partial f)^{-1}$.)\n",
    "\n",
    "5. Express $\\eta_i$ in terms of $\\mu_i$. Using the same hint as the above, compute $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$.\n",
    "\n",
    "6. Put all of the above parts together to write an expression for $\\frac{\\partial l}{\\partial \\beta_j}$. Use matrix notation to write this expression as\n",
    "$$\\nabla_{\\beta} l(\\beta) = XDV^{-1}(Y - \\mu) = 0$$\n",
    "That is, compute the matrices $D$ and $V$ such that this equation holds.\n",
    "\n",
    "7. If we use the canonical link function, how do your answers to part (6) simplify?\n",
    "\n",
    "8. Finally, compute the above likelihood equations in the case of logistic regression, and show that this is equivalent to the solution given in lecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. $l_i(\\theta_i) = \\log f(y_i|\\theta_i) = \\log h(y_i)e^{y_i\\theta_i - b(\\theta_i)} = \\log h(y_i) + \\log (y_i\\theta_i - b(\\theta_i))$, $l(\\theta) = \\sum_i l_i(\\theta_i)$\n",
    "\n",
    "2. $$\\frac{\\partial l_i}{\\partial \\beta_j} = \\frac{\\partial l_i}{\\partial \\mu_i} \\frac{\\partial \\theta_i}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}$$\n",
    "\n",
    "3. $\\frac{\\partial l_i}{\\partial \\theta_i} = y_i - b'(\\theta_i) = y_i - \\mu_i$, $\\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_j$\n",
    "\n",
    "4. $\\frac{\\partial \\theta_i}{\\partial \\mu_i} = (\\frac{\\partial \\mu_i}{\\partial \\theta_i})^{-1} = (b''(\\theta_i))^{-1}$ since $\\mu_i = b'(\\theta_i)$\n",
    "\n",
    "5. $\\frac{\\partial \\mu_i}{\\partial \\eta_i} = (\\frac{\\partial \\eta_i}{\\partial \\mu_i})^{-1} = (g'(\\mu_i))^{-1}$\n",
    "\n",
    "6. $\\frac{\\partial l_i}{\\partial \\beta_j} = (y_i - \\mu_i) (b''(\\theta_i))^{-1} (g'(\\mu_i))^{-1} x_j$.\n",
    "\n",
    "    So $$\\nabla_{\\beta} l(\\beta) = XDV^{-1}(Y - \\mu) = 0$$\n",
    "    \n",
    "    where $D$ is the diagonal matrix of  $\\frac{\\partial \\theta_i}{\\partial \\mu_i}$ and $V$ is the diagonal matrix of variances of each observation since $b''(\\theta) = Var(y|\\theta)$.\n",
    "\n",
    "7. canonical link function infers $\\theta = x^{T} \\beta = \\eta = g(\\mu)$,\n",
    "\n",
    "$$\\frac{\\partial \\mu_i}{\\partial \\eta_i} = (\\frac{\\partial \\eta_i}{\\partial \\mu_i})^{-1} = (\\frac{\\partial \\theta_i}{\\partial \\mu_i})^{-1}$$\n",
    "\n",
    "Therefore, $$\\frac{\\partial l_i}{\\partial \\beta_j} = \\frac{\\partial l_i}{\\partial \\mu_i} \\frac{\\partial \\theta_i}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j} = \\frac{\\partial l_i}{\\partial \\mu_i} \\frac{\\partial \\theta_i}{\\partial \\mu_i} (\\frac{\\partial \\theta_i}{\\partial \\mu_i})^{-1} \\frac{\\partial \\eta_i}{\\partial \\beta_j} =  \\frac{\\partial l_i}{\\partial \\mu_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}$$\n",
    "\n",
    "So $$\\nabla_{\\beta} l(\\beta) = X(Y - \\mu) = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, the link function is logit, i.e., $g(\\mu) = logit(\\mu) = \\log(\\frac{\\mu}{1 - \\mu}) = \\eta = x^{T} \\beta $\n",
    "\n",
    "$$\\mu = logit^{-1}(x^{T} \\beta) = \\frac{exp(x^{T}\\beta)}{1 + exp(x^{T}\\beta)}$$\n",
    "\n",
    "Since for logisitic regression, the observation given the probability $\\mu$ are drawn independetly from a Bernoulli distribution. So $var(y|\\theta_i) = \\mu_i(1-\\mu_i)$\n",
    "\n",
    "$$\\frac{\\partial \\theta_i}{\\partial \\mu_i} = (b''(\\theta_i))^{-1} = (\\mu_i(1-\\mu_i))^{-1}$$\n",
    "\n",
    "Also, $$\\frac{\\partial \\mu_i}{\\partial \\eta_i} = (\\frac{\\partial \\eta_i}{\\partial \\mu_i})^{-1} = (g'(\\mu_i))^{-1} = (\\frac{1}{\\mu_i(1-\\mu_i)})^{-1}$$\n",
    "\n",
    "So $$\\frac{\\partial l_i}{\\partial \\beta_j} = \\frac{\\partial l_i}{\\partial \\mu_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}$$\n",
    "\n",
    "$$\\nabla_{\\beta} l(\\beta) = X(Y - \\mu) = \\sum_i x_i[y_i - \\frac{exp(x^{T}\\beta)}{1 + exp(x^{T}\\beta)}]= 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
