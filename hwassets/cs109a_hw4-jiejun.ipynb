{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 4\n",
    "# Regularization, High Dimensionality, PCA\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook even if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately):\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "data_train = pd.read_csv('data/Bikeshare_train.csv')\n",
    "data_test = pd.read_csv('data/Bikeshare_test.csv')\n",
    "data_train = data_train.drop('Unnamed: 0', 1)\n",
    "data_test = data_test.drop('Unnamed: 0', 1)\n",
    "\n",
    "data_train = pd.get_dummies(data_train, columns=['season', 'month', 'day_of_week', 'weather'])\n",
    "data_train = data_train.drop(['season_4.0', 'month_12.0', 'day_of_week_6.0', 'weather_3.0'], 1)\n",
    "data_test = pd.get_dummies(data_test, columns=['season', 'month', 'day_of_week', 'weather'])\n",
    "data_test = data_test.drop(['season_4.0', 'month_12.0', 'day_of_week_6.0', 'weather_3.0'], 1)\n",
    "\n",
    "cp = ['temp', 'atemp', 'humidity', 'windspeed']\n",
    "cp_means = [data_train[v].mean() for v in cp]\n",
    "cp_std = [data_train[v].std() for v in cp]\n",
    "for i, v in enumerate(cp):\n",
    "    data_train[v] = (data_train[v] - cp_means[i]) / cp_std[i]\n",
    "    data_test[v] = (data_test[v] - cp_means[i]) / cp_std[i]\n",
    "\n",
    "col_names = list(data_train.columns[7:]) + list(data_train.columns[:7])\n",
    "all_predictors = col_names[:-1]\n",
    "data_train = data_train[col_names]\n",
    "data_test = data_test[col_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Bike Sharing Usage Data\n",
    "\n",
    "In this homework, we will focus on multiple linear regression, regularization, dealing with high dimensionality, and PCA. We will continue to build regression models for the Capital Bikeshare program in Washington D.C.  See Homework 3 for more information about the data.\n",
    "\n",
    "*Note: please make sure you use all the processed data from HW 3 Part (a)...you make want to save the data set on your computer and reread the csv/json file here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (f): Regularization/Penalization Methods\n",
    "\n",
    "As an alternative to selecting a subset of predictors and fitting a regression model on the subset, one can fit a linear regression model on all predictors, but shrink or regularize the coefficient estimates to make sure that the model does not \"overfit\" the training set. \n",
    "\n",
    "Use the following regularization techniques to fit linear models to the training set:\n",
    "- Ridge regression\n",
    "- Lasso regression\n",
    "    \n",
    "You may choose the shrikage parameter $\\lambda$ from the set $\\{10^{-5}, 10^{-4},...,10^{4},10^{5}\\}$ using cross-validation. In each case, \n",
    "\n",
    "- How do the estimated coefficients compare to or differ from the coefficients estimated by a plain linear regression (without shrikage penalty) in Part (b) fropm HW 3? Is there a difference between coefficients estimated by the two shrinkage methods? If so, give an explantion for the difference.\n",
    "- List the predictors that are assigned a coefficient value close to 0 (say < 1e-10) by the two methods. How closely do these predictors match the redundant predictors (if any) identified in Part (c) from HW 3?\n",
    "- Is there a difference in the way Ridge and Lasso regression assign coefficients to the predictors `temp` and `atemp`? If so, explain the reason for the difference.\n",
    "\n",
    "We next analyze the performance of the two shrinkage methods for different training sample sizes:\n",
    "- Generate random samples of sizes 100, 150, ..., 400 from the training set. You may use the following code to draw a random sample of a specified size from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  sample\n",
    "# A function to select a random sample of size k from the training set\n",
    "# Input: \n",
    "#      x (n x d array of predictors in training data)\n",
    "#      y (n x 1 array of response variable vals in training data)\n",
    "#      k (size of sample) \n",
    "# Return: \n",
    "#      chosen sample of predictors and responses\n",
    "\n",
    "def sample(x, y, k):\n",
    "    n = x.shape[0] # No. of training points\n",
    "    \n",
    "    # Choose random indices of size 'k'\n",
    "    subset_ind = np.random.choice(np.arange(n), k)\n",
    "    \n",
    "    # Get predictors and reponses with the indices\n",
    "    x_subset = x[subset_ind, :]\n",
    "    y_subset = y[subset_ind]\n",
    "    \n",
    "    return (x_subset, y_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit linear, Ridge and Lasso regression models to each of the generated sample. In each case, compute the $R^2$ score for the model on the training sample on which it was fitted, and on the test set.\n",
    "- Repeat the above experiment for 10 random trials/splits, and compute the average train and test $R^2$ across the trials for each training sample size. Also, compute the standard deviation (SD) in each case.\n",
    "- Make a plot of the mean training $R^2$ scores for the linear, Ridge and Lasso regression methods as a function of the training sample size. Also, show a confidence interval for the mean scores extending from **mean - SD** to **mean + SD**. Make a similar plot for the test $R^2$ scores.\n",
    "\n",
    "How do the training and test $R^2$ scores compare for the three methods? Give an explanation for your observations. How do the confidence intervals for the estimated $R^2$ change with training sample size? Based on the plots, which of the three methods would you recommend when one needs to fit a regression model using a small training sample?\n",
    "\n",
    "*Hint:* You may use `sklearn`'s `RidgeCV` and `LassoCV` classes to implement Ridge and Lasso regression. These classes automatically perform cross-validation to tune the parameter $\\lambda$ from a given range of values. You may use the `plt.errorbar` function to plot confidence bars for the average $R^2$ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train = data_train.iloc[:, :-1].values\n",
    "y_train = data_train.iloc[:, -1].values\n",
    "\n",
    "X_test = data_test.iloc[:, :-1].values\n",
    "y_test = data_test.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size = 100\n",
      "Ridge Regression\n",
      "The training R^2 is 0.5487638114006146.\n",
      "The test R^2 is 0.23308797024238967.\n",
      "Lasso Regression\n",
      "The training R^2 is 0.44423979748156495.\n",
      "The test R^2 is 0.2378555732472288.\n",
      "OLS Regression\n",
      "The training R^2 is 0.6447160745877005.\n",
      "The test R^2 is 0.08481065944514987.\n",
      "\n",
      "\n",
      "sample size = 150\n",
      "Ridge Regression\n",
      "The training R^2 is 0.6858922337727491.\n",
      "The test R^2 is 0.04691866833550917.\n",
      "Lasso Regression\n",
      "The training R^2 is 0.672515729536495.\n",
      "The test R^2 is 0.09335446962394656.\n",
      "OLS Regression\n",
      "The training R^2 is 0.6945494856754562.\n",
      "The test R^2 is -0.02247531302239203.\n",
      "\n",
      "\n",
      "sample size = 200\n",
      "Ridge Regression\n",
      "The training R^2 is 0.6669706605377367.\n",
      "The test R^2 is 0.18462049353828747.\n",
      "Lasso Regression\n",
      "The training R^2 is 0.6572401273011117.\n",
      "The test R^2 is 0.19806282313039147.\n",
      "OLS Regression\n",
      "The training R^2 is 0.6708368285566666.\n",
      "The test R^2 is 0.16348450681791238.\n",
      "\n",
      "\n",
      "sample size = 250\n",
      "Ridge Regression\n",
      "The training R^2 is 0.5848699457930921.\n",
      "The test R^2 is 0.2391500320703014.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression\n",
      "The training R^2 is 0.5006177093247458.\n",
      "The test R^2 is 0.24589972303538643.\n",
      "OLS Regression\n",
      "The training R^2 is 0.6083337181487707.\n",
      "The test R^2 is 0.1795934000978262.\n",
      "\n",
      "\n",
      "sample size = 300\n",
      "Ridge Regression\n",
      "The training R^2 is 0.6540900438797939.\n",
      "The test R^2 is 0.25988125839260345.\n",
      "Lasso Regression\n",
      "The training R^2 is 0.6567686134284894.\n",
      "The test R^2 is 0.2610185390505656.\n",
      "OLS Regression\n",
      "The training R^2 is 0.6571255605221005.\n",
      "The test R^2 is 0.2590679149431747.\n",
      "\n",
      "\n",
      "sample size = 350\n",
      "Ridge Regression\n",
      "The training R^2 is 0.6789896385622688.\n",
      "The test R^2 is 0.1456366351123587.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression\n",
      "The training R^2 is 0.6828145806725451.\n",
      "The test R^2 is 0.1275218208914921.\n",
      "OLS Regression\n",
      "The training R^2 is 0.6833367035077568.\n",
      "The test R^2 is 0.11633721824508747.\n",
      "\n",
      "\n",
      "sample size = 400\n",
      "Ridge Regression\n",
      "The training R^2 is 0.6293369239630187.\n",
      "The test R^2 is 0.2033300626440505.\n",
      "Lasso Regression\n",
      "The training R^2 is 0.6328439671749055.\n",
      "The test R^2 is 0.1838432044869256.\n",
      "OLS Regression\n",
      "The training R^2 is 0.6331391628852583.\n",
      "The test R^2 is 0.1757654238785692.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "alphas = [10**i for i in range(-5, 6)]\n",
    "k_list = np.arange(100, 401, 50)\n",
    "\n",
    "for k in k_list:\n",
    "    print(\"sample size = {}\".format(k))\n",
    "    X_subset, y_subset = sample(X_train, y_train, k)\n",
    "\n",
    "    ridgeRegression = RidgeCV(alphas)\n",
    "    results = ridgeRegression.fit(X_subset, y_subset)\n",
    "    print(\"Ridge Regression\")\n",
    "    print('The training R^2 is {}.'.format(results.score(X_subset, y_subset)))\n",
    "    print('The test R^2 is {}.'.format(results.score(X_test, y_test)))\n",
    "\n",
    "    lassoRegression = LassoCV(alphas = alphas)\n",
    "    results = lassoRegression.fit(X_subset, y_subset)\n",
    "    print(\"Lasso Regression\")\n",
    "    print('The training R^2 is {}.'.format(results.score(X_subset, y_subset)))\n",
    "    print('The test R^2 is {}.'.format(results.score(X_test, y_test)))\n",
    "\n",
    "    X_train_ols = sm.add_constant(X_subset)\n",
    "    X_test_ols = sm.add_constant(X_test)\n",
    "\n",
    "    result = sm.OLS(y_subset, X_train_ols).fit()\n",
    "    y_train_pred = result.predict(X_train_ols)\n",
    "    y_test_pred = result.predict(X_test_ols)\n",
    "    print(\"OLS Regression\")\n",
    "    print('The training R^2 is {}.'.format(r2_score(y_subset, y_train_pred)))\n",
    "    print('The test R^2 is {}.'.format(r2_score(y_test, y_test_pred)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Repeat for 10 random trials/splits\n",
    "\n",
    "alphas = [10**i for i in range(-5, 6)]\n",
    "k_list = np.arange(100, 401, 50)\n",
    "repeat = 10\n",
    "\n",
    "ridge_train = np.zeros([7, repeat])\n",
    "ridge_test = np.zeros([7, repeat])\n",
    "lasso_train = np.zeros([7, repeat])\n",
    "lasso_test = np.zeros([7, repeat])\n",
    "ols_train = np.zeros([7, repeat])\n",
    "ols_test = np.zeros([7, repeat])\n",
    "\n",
    "for i in range(repeat):\n",
    "    for j in range(len(k_list)):\n",
    "        X_subset, y_subset = sample(X_train, y_train, k_list[j])\n",
    "\n",
    "        ridgeRegression = RidgeCV(alphas)\n",
    "        results = ridgeRegression.fit(X_subset, y_subset)\n",
    "        ridge_train[j, i] = results.score(X_subset, y_subset)\n",
    "        ridge_test[j, i] = results.score(X_test, y_test)\n",
    "\n",
    "        lassoRegression = LassoCV(alphas = alphas)\n",
    "        results = lassoRegression.fit(X_subset, y_subset)\n",
    "        lasso_train[j, i] = results.score(X_subset, y_subset)\n",
    "        lasso_test[j, i] = results.score(X_test, y_test)\n",
    "\n",
    "        X_train_ols = sm.add_constant(X_subset)\n",
    "        X_test_ols = sm.add_constant(X_test)\n",
    "\n",
    "        result = sm.OLS(y_subset, X_train_ols).fit()\n",
    "        y_train_pred = result.predict(X_train_ols)\n",
    "        y_test_pred = result.predict(X_test_ols)\n",
    "        ols_train[j, i] = r2_score(y_subset, y_train_pred)\n",
    "        ols_test[j, i] = r2_score(y_test, y_test_pred)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x118042be0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ny564FQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11643ae48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(k_list, np.mean(ridge_train, axis = 1), 'r', label = \"ridge_train\")\n",
    "plt.plot(k_list, np.mean(ridge_train, axis = 1) + np.std(ridge_train, axis = 1), 'r--')\n",
    "plt.plot(k_list, np.mean(ridge_train, axis = 1) - np.std(ridge_train, axis = 1), 'r--')\n",
    "plt.plot(k_list, np.mean(lasso_train, axis = 1), 'g', label = \"lasso_train\")\n",
    "plt.plot(k_list, np.mean(lasso_train, axis = 1) + np.std(lasso_train, axis = 1), 'g--')\n",
    "plt.plot(k_list, np.mean(lasso_train, axis = 1) - np.std(lasso_train, axis = 1), 'g--')\n",
    "\n",
    "plt.plot(k_list, np.mean(ols_train, axis = 1), 'b', label = \"ols_train\")\n",
    "plt.plot(k_list, np.mean(ols_train, axis = 1) + np.std(ols_train, axis = 1), 'b--')\n",
    "plt.plot(k_list, np.mean(ols_train, axis = 1) - np.std(ols_train, axis = 1), 'b--')\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "xdata and ydata must be the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3505451b64f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mridge_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mridge_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ridge_test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# plt.plot(k_list, np.mean(ridge_test, axis = 1) + np.std(ridge_test, axis = 1), 'r--')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# plt.plot(k_list, np.mean(ridge_test, axis = 1) - np.std(ridge_test, axis = 1), 'r--')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plt.plot(k_list, np.mean(lasso_test, axis = 1), 'g', label = \"lasso_test\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# plt.plot(k_list, np.mean(lasso_test, axis = 1) + np.std(lasso_test, axis = 1), 'g--')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36merrorbar\u001b[0;34m(x, y, yerr, xerr, fmt, ecolor, elinewidth, capsize, barsabove, lolims, uplims, xlolims, xuplims, errorevery, capthick, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   2927\u001b[0m                           \u001b[0mxlolims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxlolims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxuplims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxuplims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2928\u001b[0m                           \u001b[0merrorevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrorevery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapthick\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcapthick\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2929\u001b[0;31m                           **kwargs)\n\u001b[0m\u001b[1;32m   2930\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2931\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36merrorbar\u001b[0;34m(self, x, y, yerr, xerr, fmt, ecolor, elinewidth, capsize, barsabove, lolims, uplims, xlolims, xuplims, errorevery, capthick, **kwargs)\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mplot_line\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2906\u001b[0m             \u001b[0mdata_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplot_line_style\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2907\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2909\u001b[0m         \u001b[0mbarcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36madd_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_line%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \"\"\"\n\u001b[0;32m-> 1815\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1816\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mget_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \"\"\"\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gwungwun/anaconda/lib/python3.6/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mrecache\u001b[0;34m(self, always)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xdata and ydata must be the same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: xdata and ydata must be the same length"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADQdJREFUeJzt3F+IpfV9x/H3p7sRGpNGiZOQ7irZljVmobHoxEiR1jS0\n7tqLJeCFGiKVwCKNIZdKocmFN81FIQT/LIsskpvsRSPJppjYQkksWNOdBf+tokxXqquCq4YUDFQG\nv72Y087pdNd5duaZmXW+7xcMzHOe38z57o/Z9z57zpyTqkKStPX91mYPIEnaGAZfkpow+JLUhMGX\npCYMviQ1YfAlqYkVg5/kcJI3kjx7lvNJ8r0k80meTnLV+GNKktZqyBX+Q8De9zm/D9g9+TgAPLD2\nsSRJY1sx+FX1GPD2+yzZD3y/Fj0BXJTkU2MNKEkax/YRvscO4JWp41OT215fvjDJARb/F8CFF154\n9RVXXDHC3UtSH8ePH3+zqmZW87VjBH+wqjoEHAKYnZ2tubm5jbx7SfrAS/Ifq/3aMX5L51Xg0qnj\nnZPbJEnnkTGCfxS4bfLbOtcCv66q//dwjiRpc634kE6SHwDXA5ckOQV8G/gQQFUdBB4BbgTmgd8A\nt6/XsJKk1Vsx+FV1ywrnC/j6aBNJktaFr7SVpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4\nktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8\nSWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+\nJDVh8CWpiUHBT7I3yQtJ5pPcfYbzH0vykyRPJTmR5PbxR5UkrcWKwU+yDbgP2AfsAW5JsmfZsq8D\nz1XVlcD1wN8luWDkWSVJazDkCv8aYL6qTlbVu8ARYP+yNQV8NEmAjwBvAwujTipJWpMhwd8BvDJ1\nfGpy27R7gc8CrwHPAN+sqveWf6MkB5LMJZk7ffr0KkeWJK3GWE/a3gA8Cfwu8IfAvUl+Z/miqjpU\nVbNVNTszMzPSXUuShhgS/FeBS6eOd05um3Y78HAtmgdeAq4YZ0RJ0hiGBP8YsDvJrskTsTcDR5et\neRn4EkCSTwKfAU6OOagkaW22r7SgqhaS3Ak8CmwDDlfViSR3TM4fBO4BHkryDBDgrqp6cx3nliSd\noxWDD1BVjwCPLLvt4NTnrwF/Pu5okqQx+UpbSWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmD\nL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITB\nl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLg\nS1ITg4KfZG+SF5LMJ7n7LGuuT/JkkhNJfjHumJKktdq+0oIk24D7gD8DTgHHkhytquem1lwE3A/s\nraqXk3xivQaWJK3OkCv8a4D5qjpZVe8CR4D9y9bcCjxcVS8DVNUb444pSVqrIcHfAbwydXxqctu0\ny4GLk/w8yfEkt53pGyU5kGQuydzp06dXN7EkaVXGetJ2O3A18BfADcDfJLl8+aKqOlRVs1U1OzMz\nM9JdS5KGWPExfOBV4NKp452T26adAt6qqneAd5I8BlwJvDjKlJKkNRtyhX8M2J1kV5ILgJuBo8vW\n/Bi4Lsn2JB8GvgA8P+6okqS1WPEKv6oWktwJPApsAw5X1Ykkd0zOH6yq55P8DHgaeA94sKqeXc/B\nJUnnJlW1KXc8Oztbc3Nzm3LfkvRBleR4Vc2u5mt9pa0kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow\n+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0Y\nfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYM\nviQ1YfAlqQmDL0lNDAp+kr1JXkgyn+Tu91n3+SQLSW4ab0RJ0hhWDH6SbcB9wD5gD3BLkj1nWfcd\n4B/HHlKStHZDrvCvAear6mRVvQscAfafYd03gB8Cb4w4nyRpJEOCvwN4Zer41OS2/5VkB/Bl4IH3\n+0ZJDiSZSzJ3+vTpc51VkrQGYz1p+13grqp67/0WVdWhqpqtqtmZmZmR7lqSNMT2AWteBS6dOt45\nuW3aLHAkCcAlwI1JFqrqR6NMKUlasyHBPwbsTrKLxdDfDNw6vaCqdv3P50keAv7B2EvS+WXF4FfV\nQpI7gUeBbcDhqjqR5I7J+YPrPKMkaQRDrvCpqkeAR5bddsbQV9Vfrn0sSdLYfKWtJDVh8CWpCYMv\nSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGX\npCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBL\nUhMGX5KaMPiS1ITBl6QmDL4kNWHwJamJQcFPsjfJC0nmk9x9hvNfSfJ0kmeSPJ7kyvFHlSStxYrB\nT7INuA/YB+wBbkmyZ9myl4A/qao/AO4BDo09qCRpbYZc4V8DzFfVyap6FzgC7J9eUFWPV9WvJodP\nADvHHVOStFZDgr8DeGXq+NTktrP5GvDTM51IciDJXJK506dPD59SkrRmoz5pm+SLLAb/rjOdr6pD\nVTVbVbMzMzNj3rUkaQXbB6x5Fbh06njn5Lb/I8nngAeBfVX11jjjSZLGMuQK/xiwO8muJBcANwNH\npxckuQx4GPhqVb04/piSpLVa8Qq/qhaS3Ak8CmwDDlfViSR3TM4fBL4FfBy4PwnAQlXNrt/YkqRz\nlaralDuenZ2tubm5TblvSfqgSnJ8tRfUvtJWkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLg\nS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHw\nJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4\nktSEwZekJgYFP8neJC8kmU9y9xnOJ8n3JuefTnLV+KNKktZixeAn2QbcB+wD9gC3JNmzbNk+YPfk\n4wDwwMhzSpLWaMgV/jXAfFWdrKp3gSPA/mVr9gPfr0VPABcl+dTIs0qS1mD7gDU7gFemjk8BXxiw\nZgfw+vSiJAdY/B8AwH8lefacpt26LgHe3OwhzhPuxRL3Yol7seQzq/3CIcEfTVUdAg4BJJmrqtmN\nvP/zlXuxxL1Y4l4scS+WJJlb7dcOeUjnVeDSqeOdk9vOdY0kaRMNCf4xYHeSXUkuAG4Gji5bcxS4\nbfLbOtcCv66q15d/I0nS5lnxIZ2qWkhyJ/AosA04XFUnktwxOX8QeAS4EZgHfgPcPuC+D6166q3H\nvVjiXixxL5a4F0tWvRepqjEHkSSdp3ylrSQ1YfAlqYl1D75vy7BkwF58ZbIHzyR5PMmVmzHnRlhp\nL6bWfT7JQpKbNnK+jTRkL5Jcn+TJJCeS/GKjZ9woA/6OfCzJT5I8NdmLIc8XfuAkOZzkjbO9VmnV\n3ayqdftg8Unefwd+D7gAeArYs2zNjcBPgQDXAr9cz5k262PgXvwRcPHk832d92Jq3T+z+EsBN232\n3Jv4c3ER8Bxw2eT4E5s99ybuxV8D35l8PgO8DVyw2bOvw178MXAV8OxZzq+qm+t9he/bMixZcS+q\n6vGq+tXk8AkWX8+wFQ35uQD4BvBD4I2NHG6DDdmLW4GHq+plgKraqvsxZC8K+GiSAB9hMfgLGzvm\n+quqx1j8s53Nqrq53sE/21sunOuareBc/5xfY/Ff8K1oxb1IsgP4Mlv/jfiG/FxcDlyc5OdJjie5\nbcOm21hD9uJe4LPAa8AzwDer6r2NGe+8sqpubuhbK2iYJF9kMfjXbfYsm+i7wF1V9d7ixVxr24Gr\ngS8Bvw38a5InqurFzR1rU9wAPAn8KfD7wD8l+Zeq+s/NHeuDYb2D79syLBn050zyOeBBYF9VvbVB\ns220IXsxCxyZxP4S4MYkC1X1o40ZccMM2YtTwFtV9Q7wTpLHgCuBrRb8IXtxO/C3tfhA9nySl4Ar\ngH/bmBHPG6vq5no/pOPbMixZcS+SXAY8DHx1i1+9rbgXVbWrqj5dVZ8G/h74qy0Yexj2d+THwHVJ\ntif5MIvvVvv8Bs+5EYbsxcss/k+HJJ9k8Z0jT27olOeHVXVzXa/wa/3eluEDZ+BefAv4OHD/5Mp2\nobbgOwQO3IsWhuxFVT2f5GfA08B7wINVteXeWnzgz8U9wENJnmHxN1Tuqqot97bJSX4AXA9ckuQU\n8G3gQ7C2bvrWCpLUhK+0laQmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpr4bz3EZ6V9PH3fAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ce80940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.errorbar(k_list, np.mean(ridge_test, axis = 1), np.std(ridge_test, axis = 1), 'r', label = \"ridge_test\")\n",
    "# plt.plot(k_list, np.mean(ridge_test, axis = 1) + np.std(ridge_test, axis = 1), 'r--')\n",
    "# plt.plot(k_list, np.mean(ridge_test, axis = 1) - np.std(ridge_test, axis = 1), 'r--')\n",
    "# plt.plot(k_list, np.mean(lasso_test, axis = 1), 'g', label = \"lasso_test\")\n",
    "# plt.plot(k_list, np.mean(lasso_test, axis = 1) + np.std(lasso_test, axis = 1), 'g--')\n",
    "# plt.plot(k_list, np.mean(lasso_test, axis = 1) - np.std(lasso_test, axis = 1), 'g--')\n",
    "\n",
    "# plt.plot(k_list, np.mean(ols_test, axis = 1), 'b', label = \"ols_test\")\n",
    "# plt.plot(k_list, np.mean(ols_test, axis = 1) + np.std(ols_test, axis = 1), 'b--')\n",
    "# plt.plot(k_list, np.mean(ols_test, axis = 1) - np.std(ols_test, axis = 1), 'b--')\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. OLS regression has higher training $R^2$ but lower test score. This might be due to over-fitting, while Ridge regression and Lasso regression has its punished term to control this problem.\n",
    "    \n",
    "    When sample size increases, the performance of these three models almost converge.\n",
    "\n",
    "2. The confidence interval narrows when sample size increases.\n",
    "\n",
    "3. Choose Ridge regression or lasso regression for small sample size since they have larger test $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (g): Polynomial & Interaction Terms\n",
    "\n",
    "Moving beyond linear models, we will now try to improve the performance of the regression model in Part (b) from HW 3 by including higher-order polynomial and interaction terms. \n",
    "\n",
    "- For each continuous predictor $X_j$, include additional polynomial terms $X^2_j$, $X^3_j$, and $X^4_j$, and fit a multiple regression model to the expanded training set. How does the $R^2$ of this model on the test set compare with that of the linear model fitted in Part (b) from HW 3? Using a t-test, find out which of estimated coefficients for the polynomial terms are statistically significant at a significance level of 5%. \n",
    "\n",
    "- Fit a multiple linear regression model with additional interaction terms $\\mathbb{I}_{month = 12} \\times temp$ and $\\mathbb{I}_{workingday = 1} \\times \\mathbb{I}_{weathersit = 1}$ and report the test $R^2$ for the fitted model. How does this compare with the $R^2$ obtained using linear model in Part (b) from HW 3? Are the estimated coefficients for the interaction terms statistically significant at a significance level of 5%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_polynomial(X, order):\n",
    "    X_new = X\n",
    "    for i in range(2, order + 1):\n",
    "        X_new = np.concatenate((X_new, X[:, -4:]**i), axis = 1)\n",
    "        \n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:, -4:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training R^2 for order 4 is 0.6696562402214015\n",
      "testing R^2 for order 4 is 0.2772384350861521\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "order = 4\n",
    "\n",
    "x_order = add_polynomial(X_train, order)\n",
    "x_order_test = add_polynomial(X_test, order)\n",
    "    \n",
    "x_order = sm.add_constant(x_order)\n",
    "x_order_test = sm.add_constant(x_order_test)\n",
    "regr_order = sm.OLS(y_train, x_order)\n",
    "results = regr_order.fit()\n",
    "print(\"training R^2 for order {} is {}\".format(order, r2_score(y_train, results.predict(x_order))))\n",
    "print(\"testing R^2 for order {} is {}\".format(order, r2_score(y_test, results.predict(x_order_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test $R^2$ is higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.625</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 07 Oct 2017</td> <th>  Prob (F-statistic):</th> <td>7.98e-50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:38:10</td>     <th>  Log-Likelihood:    </th> <td> -2790.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   331</td>      <th>  AIC:               </th> <td>   5662.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   291</td>      <th>  BIC:               </th> <td>   5814.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    39</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5429.9163</td> <td>  711.186</td> <td>    7.635</td> <td> 0.000</td> <td> 4030.197</td> <td> 6829.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>-1523.2288</td> <td>  467.580</td> <td>   -3.258</td> <td> 0.001</td> <td>-2443.496</td> <td> -602.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> -756.7981</td> <td>  536.808</td> <td>   -1.410</td> <td> 0.160</td> <td>-1813.316</td> <td>  299.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   55.5255</td> <td>  419.974</td> <td>    0.132</td> <td> 0.895</td> <td> -771.046</td> <td>  882.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>  555.6676</td> <td>  479.543</td> <td>    1.159</td> <td> 0.248</td> <td> -388.146</td> <td> 1499.481</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>  230.5990</td> <td>  478.558</td> <td>    0.482</td> <td> 0.630</td> <td> -711.275</td> <td> 1172.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>  250.8185</td> <td>  472.431</td> <td>    0.531</td> <td> 0.596</td> <td> -678.997</td> <td> 1180.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>  137.6431</td> <td>  605.236</td> <td>    0.227</td> <td> 0.820</td> <td>-1053.552</td> <td> 1328.838</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td> -481.5367</td> <td>  647.272</td> <td>   -0.744</td> <td> 0.458</td> <td>-1755.466</td> <td>  792.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td> -900.5181</td> <td>  648.671</td> <td>   -1.388</td> <td> 0.166</td> <td>-2177.201</td> <td>  376.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td> -861.3206</td> <td>  653.672</td> <td>   -1.318</td> <td> 0.189</td> <td>-2147.844</td> <td>  425.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>-1160.2713</td> <td>  644.832</td> <td>   -1.799</td> <td> 0.073</td> <td>-2429.397</td> <td>  108.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td> -517.7332</td> <td>  527.018</td> <td>   -0.982</td> <td> 0.327</td> <td>-1554.984</td> <td>  519.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td> -370.2035</td> <td>  420.771</td> <td>   -0.880</td> <td> 0.380</td> <td>-1198.343</td> <td>  457.936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td> -269.8653</td> <td>  380.406</td> <td>   -0.709</td> <td> 0.479</td> <td>-1018.560</td> <td>  478.830</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td> -471.0834</td> <td>  246.557</td> <td>   -1.911</td> <td> 0.057</td> <td> -956.345</td> <td>   14.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td> -227.9217</td> <td>  157.372</td> <td>   -1.448</td> <td> 0.149</td> <td> -537.653</td> <td>   81.809</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td> -268.0232</td> <td>  186.488</td> <td>   -1.437</td> <td> 0.152</td> <td> -635.060</td> <td>   99.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   13.1360</td> <td>  196.966</td> <td>    0.067</td> <td> 0.947</td> <td> -374.522</td> <td>  400.794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td> -104.0028</td> <td>  189.352</td> <td>   -0.549</td> <td> 0.583</td> <td> -476.675</td> <td>  268.669</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   75.3419</td> <td>  184.193</td> <td>    0.409</td> <td> 0.683</td> <td> -287.178</td> <td>  437.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td> 1043.9997</td> <td>  546.051</td> <td>    1.912</td> <td> 0.057</td> <td>  -30.710</td> <td> 2118.709</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td> 1103.0116</td> <td>  499.023</td> <td>    2.210</td> <td> 0.028</td> <td>  120.859</td> <td> 2085.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td> -526.2557</td> <td>  372.773</td> <td>   -1.412</td> <td> 0.159</td> <td>-1259.929</td> <td>  207.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>   14.7858</td> <td>  159.059</td> <td>    0.093</td> <td> 0.926</td> <td> -298.265</td> <td>  327.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>  771.4866</td> <td>  760.117</td> <td>    1.015</td> <td> 0.311</td> <td> -724.536</td> <td> 2267.510</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>  897.2756</td> <td>  713.172</td> <td>    1.258</td> <td> 0.209</td> <td> -506.353</td> <td> 2300.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td> -668.9145</td> <td>  157.356</td> <td>   -4.251</td> <td> 0.000</td> <td> -978.615</td> <td> -359.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td> -446.5085</td> <td>  148.929</td> <td>   -2.998</td> <td> 0.003</td> <td> -739.623</td> <td> -153.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>-1811.0180</td> <td>  816.910</td> <td>   -2.217</td> <td> 0.027</td> <td>-3418.820</td> <td> -203.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td> 1175.5005</td> <td>  788.864</td> <td>    1.490</td> <td> 0.137</td> <td> -377.102</td> <td> 2728.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>  -53.6709</td> <td>  155.383</td> <td>   -0.345</td> <td> 0.730</td> <td> -359.488</td> <td>  252.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>  -34.1653</td> <td>  126.952</td> <td>   -0.269</td> <td> 0.788</td> <td> -284.026</td> <td>  215.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    8.6078</td> <td>  275.731</td> <td>    0.031</td> <td> 0.975</td> <td> -534.071</td> <td>  551.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td> -303.9358</td> <td>  246.097</td> <td>   -1.235</td> <td> 0.218</td> <td> -788.292</td> <td>  180.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>  -16.0576</td> <td>   44.892</td> <td>   -0.358</td> <td> 0.721</td> <td> -104.412</td> <td>   72.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   44.8339</td> <td>   65.459</td> <td>    0.685</td> <td> 0.494</td> <td>  -83.999</td> <td>  173.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>  -45.1910</td> <td>  171.419</td> <td>   -0.264</td> <td> 0.792</td> <td> -382.570</td> <td>  292.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>  -20.7686</td> <td>  147.605</td> <td>   -0.141</td> <td> 0.888</td> <td> -311.276</td> <td>  269.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>  -24.8367</td> <td>   31.481</td> <td>   -0.789</td> <td> 0.431</td> <td>  -86.796</td> <td>   37.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>  -20.1769</td> <td>   30.327</td> <td>   -0.665</td> <td> 0.506</td> <td>  -79.864</td> <td>   39.510</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>29.995</td> <th>  Durbin-Watson:     </th> <td>   1.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  10.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.094</td> <th>  Prob(JB):          </th> <td> 0.00609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.161</td> <th>  Cond. No.          </th> <td>1.36e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.670\n",
       "Model:                            OLS   Adj. R-squared:                  0.625\n",
       "Method:                 Least Squares   F-statistic:                     15.13\n",
       "Date:                Sat, 07 Oct 2017   Prob (F-statistic):           7.98e-50\n",
       "Time:                        15:38:10   Log-Likelihood:                -2790.9\n",
       "No. Observations:                 331   AIC:                             5662.\n",
       "Df Residuals:                     291   BIC:                             5814.\n",
       "Df Model:                          39                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5429.9163    711.186      7.635      0.000    4030.197    6829.636\n",
       "x1         -1523.2288    467.580     -3.258      0.001   -2443.496    -602.961\n",
       "x2          -756.7981    536.808     -1.410      0.160   -1813.316     299.720\n",
       "x3            55.5255    419.974      0.132      0.895    -771.046     882.097\n",
       "x4           555.6676    479.543      1.159      0.248    -388.146    1499.481\n",
       "x5           230.5990    478.558      0.482      0.630    -711.275    1172.473\n",
       "x6           250.8185    472.431      0.531      0.596    -678.997    1180.634\n",
       "x7           137.6431    605.236      0.227      0.820   -1053.552    1328.838\n",
       "x8          -481.5367    647.272     -0.744      0.458   -1755.466     792.392\n",
       "x9          -900.5181    648.671     -1.388      0.166   -2177.201     376.164\n",
       "x10         -861.3206    653.672     -1.318      0.189   -2147.844     425.203\n",
       "x11        -1160.2713    644.832     -1.799      0.073   -2429.397     108.854\n",
       "x12         -517.7332    527.018     -0.982      0.327   -1554.984     519.518\n",
       "x13         -370.2035    420.771     -0.880      0.380   -1198.343     457.936\n",
       "x14         -269.8653    380.406     -0.709      0.479   -1018.560     478.830\n",
       "x15         -471.0834    246.557     -1.911      0.057    -956.345      14.178\n",
       "x16         -227.9217    157.372     -1.448      0.149    -537.653      81.809\n",
       "x17         -268.0232    186.488     -1.437      0.152    -635.060      99.014\n",
       "x18           13.1360    196.966      0.067      0.947    -374.522     400.794\n",
       "x19         -104.0028    189.352     -0.549      0.583    -476.675     268.669\n",
       "x20           75.3419    184.193      0.409      0.683    -287.178     437.861\n",
       "x21         1043.9997    546.051      1.912      0.057     -30.710    2118.709\n",
       "x22         1103.0116    499.023      2.210      0.028     120.859    2085.164\n",
       "x23         -526.2557    372.773     -1.412      0.159   -1259.929     207.418\n",
       "x24           14.7858    159.059      0.093      0.926    -298.265     327.837\n",
       "x25          771.4866    760.117      1.015      0.311    -724.536    2267.510\n",
       "x26          897.2756    713.172      1.258      0.209    -506.353    2300.904\n",
       "x27         -668.9145    157.356     -4.251      0.000    -978.615    -359.214\n",
       "x28         -446.5085    148.929     -2.998      0.003    -739.623    -153.394\n",
       "x29        -1811.0180    816.910     -2.217      0.027   -3418.820    -203.216\n",
       "x30         1175.5005    788.864      1.490      0.137    -377.102    2728.103\n",
       "x31          -53.6709    155.383     -0.345      0.730    -359.488     252.146\n",
       "x32          -34.1653    126.952     -0.269      0.788    -284.026     215.695\n",
       "x33            8.6078    275.731      0.031      0.975    -534.071     551.287\n",
       "x34         -303.9358    246.097     -1.235      0.218    -788.292     180.420\n",
       "x35          -16.0576     44.892     -0.358      0.721    -104.412      72.297\n",
       "x36           44.8339     65.459      0.685      0.494     -83.999     173.667\n",
       "x37          -45.1910    171.419     -0.264      0.792    -382.570     292.188\n",
       "x38          -20.7686    147.605     -0.141      0.888    -311.276     269.739\n",
       "x39          -24.8367     31.481     -0.789      0.431     -86.796      37.122\n",
       "x40          -20.1769     30.327     -0.665      0.506     -79.864      39.510\n",
       "==============================================================================\n",
       "Omnibus:                       29.995   Durbin-Watson:                   1.959\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               10.202\n",
       "Skew:                          -0.094   Prob(JB):                      0.00609\n",
       "Kurtosis:                       2.161   Cond. No.                     1.36e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.08e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 significant predictors: const, season_1.0, weather 2.0, humidity, windspeed, temp^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = data_train['weather_1.0'] * data_train['workingday']\n",
    "x_inter = np.concatenate((X_train, tmp.values.reshape(-1, 1)), axis = 1)\n",
    "tmp_test = data_test['weather_1.0'] * data_test['workingday']\n",
    "x_inter_test = np.concatenate((X_test, tmp_test.values.reshape(-1, 1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_tmp = pd.read_csv('data/Bikeshare_train.csv')\n",
    "data_test_tmp = pd.read_csv('data/Bikeshare_test.csv')\n",
    "data_train_tmp = pd.get_dummies(data_train_tmp, columns=['month'])\n",
    "data_test_tmp = pd.get_dummies(data_test_tmp, columns=['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = data_train_tmp['month_12.0'] * data_train['temp']\n",
    "x_inter = np.concatenate((x_inter, tmp.values.reshape(-1, 1)), axis = 1)\n",
    "tmp_test = data_test_tmp['month_12.0'] * data_test['temp']\n",
    "x_inter_test = np.concatenate((x_inter_test, tmp_test.values.reshape(-1, 1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training R^2 for order 4 is 0.580381348480301\n",
      "testing R^2 for order 4 is 0.2685181896268102\n"
     ]
    }
   ],
   "source": [
    "x_inter = sm.add_constant(x_inter)\n",
    "x_inter_test = sm.add_constant(x_inter_test)\n",
    "regr_order = sm.OLS(y_train, x_inter)\n",
    "results = regr_order.fit()\n",
    "print(\"training R^2 for order {} is {}\".format(order, r2_score(y_train, results.predict(x_inter))))\n",
    "print(\"testing R^2 for order {} is {}\".format(order, r2_score(y_test, results.predict(x_inter_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test $R^2$ is higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   14.36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 07 Oct 2017</td> <th>  Prob (F-statistic):</th> <td>2.43e-41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:38:10</td>     <th>  Log-Likelihood:    </th> <td> -2830.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   331</td>      <th>  AIC:               </th> <td>   5721.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   301</td>      <th>  BIC:               </th> <td>   5835.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    29</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4734.0152</td> <td>  918.011</td> <td>    5.157</td> <td> 0.000</td> <td> 2927.483</td> <td> 6540.547</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> -884.3187</td> <td>  550.763</td> <td>   -1.606</td> <td> 0.109</td> <td>-1968.152</td> <td>  199.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> -110.9431</td> <td>  587.150</td> <td>   -0.189</td> <td> 0.850</td> <td>-1266.381</td> <td> 1044.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>  -75.5976</td> <td>  453.682</td> <td>   -0.167</td> <td> 0.868</td> <td> -968.388</td> <td>  817.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>-1211.6733</td> <td> 1040.400</td> <td>   -1.165</td> <td> 0.245</td> <td>-3259.052</td> <td>  835.706</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>-1095.9921</td> <td> 1020.067</td> <td>   -1.074</td> <td> 0.283</td> <td>-3103.357</td> <td>  911.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td> -896.4352</td> <td>  975.450</td> <td>   -0.919</td> <td> 0.359</td> <td>-2816.000</td> <td> 1023.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td> -705.5649</td> <td>  997.870</td> <td>   -0.707</td> <td> 0.480</td> <td>-2669.249</td> <td> 1258.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>-1011.9024</td> <td> 1004.051</td> <td>   -1.008</td> <td> 0.314</td> <td>-2987.750</td> <td>  963.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>-1737.2977</td> <td>  980.173</td> <td>   -1.772</td> <td> 0.077</td> <td>-3666.157</td> <td>  191.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>-2140.0440</td> <td>  941.775</td> <td>   -2.272</td> <td> 0.024</td> <td>-3993.341</td> <td> -286.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>-1640.2677</td> <td>  934.999</td> <td>   -1.754</td> <td> 0.080</td> <td>-3480.230</td> <td>  199.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td> -421.5095</td> <td>  838.850</td> <td>   -0.502</td> <td> 0.616</td> <td>-2072.263</td> <td> 1229.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td> -290.2825</td> <td>  766.708</td> <td>   -0.379</td> <td> 0.705</td> <td>-1799.069</td> <td> 1218.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td> -681.9367</td> <td>  777.020</td> <td>   -0.878</td> <td> 0.381</td> <td>-2211.016</td> <td>  847.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td> -423.1840</td> <td>  269.763</td> <td>   -1.569</td> <td> 0.118</td> <td> -954.044</td> <td>  107.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td> -272.5310</td> <td>  174.283</td> <td>   -1.564</td> <td> 0.119</td> <td> -615.499</td> <td>   70.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td> -357.2100</td> <td>  206.132</td> <td>   -1.733</td> <td> 0.084</td> <td> -762.851</td> <td>   48.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   -7.7484</td> <td>  217.339</td> <td>   -0.036</td> <td> 0.972</td> <td> -435.446</td> <td>  419.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>  -89.8205</td> <td>  211.342</td> <td>   -0.425</td> <td> 0.671</td> <td> -505.716</td> <td>  326.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>  -43.0576</td> <td>  203.513</td> <td>   -0.212</td> <td> 0.833</td> <td> -443.546</td> <td>  357.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td> 1339.9147</td> <td>  590.803</td> <td>    2.268</td> <td> 0.024</td> <td>  177.287</td> <td> 2502.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td> 1563.2213</td> <td>  477.700</td> <td>    3.272</td> <td> 0.001</td> <td>  623.167</td> <td> 2503.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td> -571.0206</td> <td>  406.951</td> <td>   -1.403</td> <td> 0.162</td> <td>-1371.851</td> <td>  229.809</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td> -199.3469</td> <td>  265.959</td> <td>   -0.750</td> <td> 0.454</td> <td> -722.720</td> <td>  324.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>  906.9589</td> <td>  475.169</td> <td>    1.909</td> <td> 0.057</td> <td>  -28.114</td> <td> 1842.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>  274.7706</td> <td>  429.787</td> <td>    0.639</td> <td> 0.523</td> <td> -570.997</td> <td> 1120.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td> -574.1179</td> <td>  113.992</td> <td>   -5.036</td> <td> 0.000</td> <td> -798.441</td> <td> -349.795</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td> -275.7457</td> <td>   81.737</td> <td>   -3.374</td> <td> 0.001</td> <td> -436.593</td> <td> -114.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>  313.2194</td> <td>  352.789</td> <td>    0.888</td> <td> 0.375</td> <td> -381.025</td> <td> 1007.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td> 1045.1338</td> <td>  729.703</td> <td>    1.432</td> <td> 0.153</td> <td> -390.832</td> <td> 2481.099</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>33.048</td> <th>  Durbin-Watson:     </th> <td>   1.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  10.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.037</td> <th>  Prob(JB):          </th> <td> 0.00562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.136</td> <th>  Cond. No.          </th> <td>2.98e+15</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.580\n",
       "Model:                            OLS   Adj. R-squared:                  0.540\n",
       "Method:                 Least Squares   F-statistic:                     14.36\n",
       "Date:                Sat, 07 Oct 2017   Prob (F-statistic):           2.43e-41\n",
       "Time:                        15:38:10   Log-Likelihood:                -2830.5\n",
       "No. Observations:                 331   AIC:                             5721.\n",
       "Df Residuals:                     301   BIC:                             5835.\n",
       "Df Model:                          29                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4734.0152    918.011      5.157      0.000    2927.483    6540.547\n",
       "x1          -884.3187    550.763     -1.606      0.109   -1968.152     199.515\n",
       "x2          -110.9431    587.150     -0.189      0.850   -1266.381    1044.495\n",
       "x3           -75.5976    453.682     -0.167      0.868    -968.388     817.193\n",
       "x4         -1211.6733   1040.400     -1.165      0.245   -3259.052     835.706\n",
       "x5         -1095.9921   1020.067     -1.074      0.283   -3103.357     911.373\n",
       "x6          -896.4352    975.450     -0.919      0.359   -2816.000    1023.129\n",
       "x7          -705.5649    997.870     -0.707      0.480   -2669.249    1258.119\n",
       "x8         -1011.9024   1004.051     -1.008      0.314   -2987.750     963.945\n",
       "x9         -1737.2977    980.173     -1.772      0.077   -3666.157     191.562\n",
       "x10        -2140.0440    941.775     -2.272      0.024   -3993.341    -286.747\n",
       "x11        -1640.2677    934.999     -1.754      0.080   -3480.230     199.695\n",
       "x12         -421.5095    838.850     -0.502      0.616   -2072.263    1229.244\n",
       "x13         -290.2825    766.708     -0.379      0.705   -1799.069    1218.504\n",
       "x14         -681.9367    777.020     -0.878      0.381   -2211.016     847.142\n",
       "x15         -423.1840    269.763     -1.569      0.118    -954.044     107.676\n",
       "x16         -272.5310    174.283     -1.564      0.119    -615.499      70.437\n",
       "x17         -357.2100    206.132     -1.733      0.084    -762.851      48.431\n",
       "x18           -7.7484    217.339     -0.036      0.972    -435.446     419.949\n",
       "x19          -89.8205    211.342     -0.425      0.671    -505.716     326.075\n",
       "x20          -43.0576    203.513     -0.212      0.833    -443.546     357.431\n",
       "x21         1339.9147    590.803      2.268      0.024     177.287    2502.542\n",
       "x22         1563.2213    477.700      3.272      0.001     623.167    2503.275\n",
       "x23         -571.0206    406.951     -1.403      0.162   -1371.851     229.809\n",
       "x24         -199.3469    265.959     -0.750      0.454    -722.720     324.027\n",
       "x25          906.9589    475.169      1.909      0.057     -28.114    1842.032\n",
       "x26          274.7706    429.787      0.639      0.523    -570.997    1120.538\n",
       "x27         -574.1179    113.992     -5.036      0.000    -798.441    -349.795\n",
       "x28         -275.7457     81.737     -3.374      0.001    -436.593    -114.898\n",
       "x29          313.2194    352.789      0.888      0.375    -381.025    1007.464\n",
       "x30         1045.1338    729.703      1.432      0.153    -390.832    2481.099\n",
       "==============================================================================\n",
       "Omnibus:                       33.048   Durbin-Watson:                   1.921\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               10.362\n",
       "Skew:                           0.037   Prob(JB):                      0.00562\n",
       "Kurtosis:                       2.136   Cond. No.                     2.98e+15\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.01e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 significant predictors: const, month_7.0, weather 1.0, weather 2.0, humidity, windspeed.\n",
    "\n",
    "None of the interactive terms and their relative predictors are significant. The reason might be multi-colinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (h): PCA to deal with high dimensionality\n",
    "\n",
    "We would like to fit a model to include all main effects, polynomial terms up to the $4^{th}$ order, and all interactions between all possible predictors and polynomial terms (not including the interactions between $X^1_j$, $X^2_j$, $X^3_j$, and $X^4_j$ as they would just create higher order polynomial terms).  \n",
    "\n",
    "- Create an expanded training set including all the desired terms mentioned above.  What are the dimensions of this 'design matrix' of all the predictor variables?   What are the issues with attempting to fit a regression model using all of these predictors?\n",
    "\n",
    "- Instead of using the usual approaches for model selection, let's instead use principal components analysis (PCA) to fit the model.  First, create the principal component vectors in python (consider: should you normalize first?).  Then fit 5 different regression models: (1) using just the first PCA vector, (2) using the first two PCA vectors, (3) using the first three PCA vectors, etc...  Briefly summarize how these models compare in the training set.\n",
    "\n",
    "- Use the test set to decide which of the 5 models above is best to predict out of sample.  How does this model compare to the previous models you've fit?  What are the interpretations of this model's coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_polynomial(X, order):\n",
    "    X_new = X[:, -4:] ** 2\n",
    "    for i in range(3, order + 1):\n",
    "        X_new = np.concatenate((X_new, X[:, -4:]**i), axis = 1)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "interactive = PolynomialFeatures(degree=2, interaction_only=True).fit_transform(X_train)\n",
    "interactive = np.concatenate((interactive, add_polynomial(X_train, 4)), axis = 1)\n",
    "interactive_test = PolynomialFeatures(degree=2, interaction_only=True).fit_transform(X_test)\n",
    "interactive_test = np.concatenate((interactive_test, add_polynomial(X_test, 4)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331, 419)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-linearity. Also the computation cost is too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "interactive = normalize(interactive, axis= 0)\n",
    "interactive_test = normalize(interactive_test, axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.346449011502\n",
      "0.208711780599\n",
      "2\n",
      "0.348582194888\n",
      "0.209250926546\n",
      "3\n",
      "0.368181260459\n",
      "0.215809391337\n",
      "4\n",
      "0.380668864263\n",
      "0.213844871712\n",
      "5\n",
      "0.381133501963\n",
      "0.21220982517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "for i in range(1, 6):\n",
    "    pca = PCA(n_components= i)\n",
    "    pca.fit(interactive)\n",
    "    x_train_pca = pca.transform(interactive)\n",
    "    x_test_pca = pca.transform(interactive_test)\n",
    "    x_train_pca = sm.add_constant(x_train_pca)\n",
    "    x_test_pca = sm.add_constant(x_test_pca)\n",
    "    result = sm.OLS(y_train, x_train_pca).fit()\n",
    "    print(i)\n",
    "    print(r2_score(y_train, result.predict(x_train_pca)))\n",
    "    print(r2_score(y_test, result.predict(x_test_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should normalize since we are comparing their ability to explain variety in the dataset. So we have to normalize, otherwise PCA can't select correct variables through unnormalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the increment of components, the training score increases.\n",
    "\n",
    "The test score are worse than the previous models.\n",
    "The coefficients refer to the increment of response as the combined predictors, which explain most of the variance ratio, increases 1 unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (i): Beyond Squared Error\n",
    "\n",
    "We have seen in class that the multiple linear regression method optimizes the Mean Squared Error (MSE) on the training set. Consider the following alternate evaluation metric, referred to as the Root Mean Squared Logarthmic Error (RMSLE):\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (log(y_i+1) - log(\\hat{y}_i+1))^2}.\n",
    "$$\n",
    "\n",
    "The *lower* the RMSLE the *better* is the performance of a model. The RMSLE penalizes errors on smaller responses more heavily than errors on larger responses. For example, the RMSLE penalizes a prediction of $\\hat{y} = 15$ for a true response of $y=10$ more heavily than a prediction of $\\hat{y} = 105$ for a true response of $100$, though the difference in predicted and true responses are the same in both cases. \n",
    "\n",
    "This is a natural evaluation metric for bike share demand prediction, as in this application, it is more important that the prediction model is accurate on days where the demand is low (so that the few customers who arrive are served satisfactorily), compared to days on which the demand is high (when it is less damaging to lose out on some customers).\n",
    "\n",
    "The following code computes the RMSLE for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  rmsle\n",
    "# A function for evaluating Root Mean Squared Logarithmic Error (RMSLE)\n",
    "# of the linear regression model on a data set\n",
    "# Input: \n",
    "#      y_test (n x 1 array of response variable vals in testing data)\n",
    "#      y_pred (n x 1 array of response variable vals in testing data)\n",
    "# Return: \n",
    "#      RMSLE (float) \n",
    "\n",
    "def rmsle(y, y_pred):     \n",
    "    # Evaluate sqaured error, against target labels\n",
    "    # rmsle = \\sqrt(1/n \\sum_i (log (y[i]+1) - log (y_pred[i]+1))^2)\n",
    "    rmsle_ = np.sqrt(np.mean(np.square(np.log(y+1) - np.log(y_pred+1))))\n",
    "    \n",
    "    return rmsle_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above code to compute the training and test RMSLE for the polynomial regression model you fit in Part (g). \n",
    "\n",
    "You are required to develop a strategy to fit a regression model by optimizing the RMSLE on the training set. Give a justification for your proposed approach. Does the model fitted using your approach yield lower train RMSLE than the model in Part (g)? How about the test RMSLE of the new model? \n",
    "\n",
    "**Note:** We do not require you to implement a new regression solver for RMSLE. Instead, we ask you to think about ways to use existing built-in functions to fit a model that performs well on RMSLE. Your regression model may use the same polynomial terms used in Part (g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RMSLE: 0.7214576597005684\n",
      "testing RMSLE: 0.8576183145361487\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "result = sm.OLS(y_train, x_order).fit()\n",
    "y_pred = result.predict(x_order)\n",
    "y_pred[y_pred < -1] = 0\n",
    "y_pred_test = result.predict(x_order_test)\n",
    "y_pred_test[y_pred_test < -1] = 0\n",
    "print(\"training RMSLE: {}\" .format(rmsle(y_train, y_pred)))\n",
    "print(\"testing RMSLE: {}\" .format(rmsle(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coeff0 = result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_rmsle(coeff, X=x_order, y=y_train):\n",
    "    y_pred = np.dot(X, coeff)\n",
    "    y_pred[y_pred < -1] = 0\n",
    "    return rmsle(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin\n",
    "coeff1 = fmin(loss_rmsle, coeff0, maxiter=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent method using optimize.fmin function in scipy package to choose beta that minize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85170554756544981"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rmsle(coeff1, x_order_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16140342401960106"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, np.dot(x_order_test, coeff1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rmsle score for new model is slightly lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (j): Dealing with Erroneous Labels\n",
    "\n",
    "Due to occasional system crashes, some of the bike counts reported in the data set have been recorded manually. These counts are not very unreliable and are prone to errors. It is known that roughly 5% of the labels in the training set are erroneous (i.e. can be arbitrarily different from the true counts), while all the labels in the test set were confirmed to be accurate. Unfortunately, the identities of the erroneous records in the training set are not available. Can this information about presence of 5% errors in the training set labels (without details about the specific identities of the erroneous rows) be used to improve the performance of the model in Part (g)? Note that we are interested in improving the $R^2$ performance of the model on the test set (not the training $R^2$ score). \n",
    "\n",
    "As a final task, we require you to come up with a strategy to fit a regression model, taking into account the errors in the training set labels. Explain the intuition behind your approach (we do not expect a detailed mathematical justification). Use your approach to fit a regression model on the training set, and compare its test $R^2$ with the model in Part (g).\n",
    "\n",
    "**Note:** Again, we do not require you to implement a new regression solver for handling erroneous labels. It is sufficient that you to come up with an approach that uses existing built-in functions. Your regression model may use the same polynomial terms used in Part (g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "def loss_rmsle_thres(coeff, X=x_order, y=y_train, thres = 0.05):\n",
    "    y_pred = np.dot(X, coeff)\n",
    "    r = abs(y_pred - y)\n",
    "    ind = np.argsort(r)[:(int)(len(r) * (1 - 0.05))]\n",
    "    y_pred = y_pred[ind]\n",
    "    y = y[ind]\n",
    "    y_pred[y_pred < -1] = 0\n",
    "    return rmsle(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.253879\n",
      "         Iterations: 35066\n",
      "         Function evaluations: 42211\n"
     ]
    }
   ],
   "source": [
    "coeff2 = fmin(loss_rmsle_thres, coeff0, maxiter=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28010031005526859"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, np.dot(x_order_test, coeff2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RMSLE in part (i) is worse than the previous model based on MSE, while RMSLE in this part is higher. That means MSE is relatively robust while RMSLE replies on the accuracy of data set. Another reason might be we replace the negative result to 0 so this process might loss some information. But when we deal with the error part, the optimization of RMSLE helps us to find better coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APCOMP209a - Homework Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Question 1: Student's t MLE\n",
    "\n",
    "Use Maximum Likelihood Estimation to generate a linear regression model on the data provided in ``beerdata.csv`` considering two statistical models for noise: a) iid Normal and b) iid Student's t-distribution with $\\nu=5$ and scale factor =0.5.  \n",
    "\n",
    "Compare the two models performances and comment why it is perhaps appropriate to use the Student's t-distribution instead of the Normal? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HINTS:\n",
    "1. Use the probability density function for the Student's t distribution  with location  and scale factor .\n",
    "2. If the MLE regressions coefficients can not be derived analytically consider numerical methods.\n",
    "3. You can use sklearn or statsmodel for the Normal case \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/beerdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (continued from HW2) \n",
    "\n",
    "Read sections 1 and 2 of this [paper](https://www.researchgate.net/profile/Roberto_Togneri/publication/45094554_Linear_Regression_for_Face_Recognition/links/09e4150d243bd8b987000000/Linear-Regression-for-Face-Recognition.pdf). \n",
    "\n",
    "Briefly, the model leverages the concept that \"patterns from a single-object class lie on a linear subspace.\"   It also makes use of the idea of linear regression as a problem about projections.  In this case, given a vector $y$, the goal is to find the subspace induced by $\\mathrm{Col} \\, \\mathbf X$ that produced the 'closest' projection vector $\\widehat y$ to the original $y$.  \n",
    "\n",
    "### Question 2a\n",
    "\n",
    "As discussed in the paper, our face dataset contains cleaned images of faces belonging to different people. Assuming that patterns (faces) from one class (person) are elements of the same subspace, let's try to classify an unknown face using the method presented in the paper.  For each class $i$, we need to:\n",
    "\n",
    "1. construct the $\\mathbf H_i$ hat matrix from known faces, being careful to follow the column concatenation step described in the paper to convert an image into its vector representation;\n",
    "2. calculate the predicted $\\widehat y_i$, the closest vector in $\\mathrm{Col} \\, \\mathbf X_i$ to $y$; and\n",
    "3. calculate the magnitude of the difference vector between $y$ and $\\widehat y_i$.\n",
    "\n",
    "You should then be able to make a classification decision.\n",
    "\n",
    "**Notes:**\n",
    "- Use the provided code to download and re-sample the dataset.\n",
    "- Follow the normalisation step in the paper to ensure the \"maximum pixel value is 1\".\n",
    "- Your classifier should have approximately an 80% accuracy.\n",
    "- Use the image plotting library of matplotlib to display one (or two) correctly classified faces and the known faces.\n",
    "- Use the image plotting library of matplotlib to display one (or two) incorrectly classified faces and the known faces.\n",
    "    \n",
    "### Question 2b - Significant Faces\n",
    "Select an example of a correctly classified face. Use statsmodels to investigate the most predictive columns (faces) that the model used in this regression:\n",
    "\n",
    "(i) Which columns (i.e. faces) make the highest contribution to the projection?\n",
    "\n",
    "(ii) Which columns (i.e. faces) are the least useful in making this projection?\n",
    "\n",
    "Plot the correctly assigned face, and the two faces from the questions (i) and (ii). What do you notice about these faces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwungwun/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "# Note that you may need to run the following command to install Python Image Library (PIL)\n",
    "#pip install Pillow\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# starter functions provided to students\n",
    "def rgb2gray(rgb):\n",
    "    '''\n",
    "    function to convert RGB image to gray scale\n",
    "    accepts 3D numpy array and returns 2D array with same dimensions\n",
    "    as the first two dimensions of input\n",
    "    '''\n",
    "    \n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def fetch_and_read_data(shape=(50,30)):\n",
    "    \n",
    "    '''\n",
    "    Function to download image data, store in a local folder (note this is 18.4mb), only download the data when\n",
    "    the local folder is not present, read in the images, downsample them to the specified shape (default = (50x30) (rows x cols))\n",
    "    and finally split them into a four tuple return object.\n",
    "    \n",
    "    Returns:\n",
    "        - 1) training image data (i.e. images that should form the predictor matrix in your solution)\n",
    "        - 2) training image data labels (i.e. labels from 1 to 50 that identify which face (1) belongs to)\n",
    "        - 3) testing image data (i.e. data that you should use to try and classify - note this forms the predictor variable in your regression)\n",
    "        - 4) testing image data labels (i.e. the labels for (3) - this is to allow you to evaluate your model)\n",
    "    \n",
    "    ___________________\n",
    "    Aside:\n",
    "    If you want to change the sampling dimensions of your data, pass the shape = (x,y) argument to the method where\n",
    "    y is the number of columns and x is the number of rows in the image.\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists('./cropped_faces'):\n",
    "        url = urllib.request.urlopen(\"http://www.anefian.com/research/GTdb_crop.zip\")\n",
    "        \n",
    "        zipfile = ZipFile(BytesIO(url.read()))\n",
    "        zipfile.extractall()\n",
    "     \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    files = os.listdir('cropped_faces')\n",
    "    for f in files:\n",
    "        if '.jpg' in f:\n",
    "            image = Image.open('cropped_faces/' + f)\n",
    "            image = image.resize((shape[1], shape[0]))\n",
    "            data.append(rgb2gray(np.asarray(image)))\n",
    "            labels.append(int(f.split('_')[0][1:]) - 1)\n",
    "            \n",
    "    data = np.array(data)\n",
    "    \n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size=0.2, stratify=labels)\n",
    "    return np.array(trainX), np.array(testX), np.array(trainY), np.array(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAEyCAYAAADKlMtrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cf47b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# starter code for the students\n",
    "train_dataset, test_dataset, train_labels, test_labels = fetch_and_read_data()\n",
    "\n",
    "# code to plot some of the images\n",
    "fig, axes = plt.subplots(2,4,figsize=(10,5))\n",
    "axes = axes.flatten()\n",
    "[axes[i].imshow(train_dataset[i], cmap='gray') for i in range(len(axes))]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  89.619,   85.858,   74.808, ...,   49.004,   50.847,   53.847],\n",
       "        [  99.217,   96.836,   27.981, ...,   45.118,   49.075,   47.847],\n",
       "        [  95.292,   35.943,   23.039, ...,   38.075,   48.217,   43.157],\n",
       "        ..., \n",
       "        [  58.116,   59.757,   65.176, ...,  157.333,  159.735,  137.963],\n",
       "        [  64.116,   53.757,   66.817, ...,  161.273,  157.974,  142.963],\n",
       "        [  56.116,   52.757,   73.273, ...,  169.273,  169.974,  134.963]],\n",
       "\n",
       "       [[  44.716,   63.716,   68.83 , ...,   29.171,   28.943,   30.242],\n",
       "        [  51.716,   58.118,   61.759, ...,   39.285,   29.943,   35.943],\n",
       "        [  71.716,   60.759,   65.275, ...,   35.605,   31.932,   36.943],\n",
       "        ..., \n",
       "        [  35.943,   58.302,   73.096, ...,   48.915,   53.932,   68.074],\n",
       "        [  52.16 ,   35.16 ,   39.943, ...,   35.687,   68.302,   60.704],\n",
       "        [  45.16 ,   35.16 ,   40.943, ...,   38.687,   67.302,   54.704]],\n",
       "\n",
       "       [[  56.256,   50.174,   61.217, ...,   39.726,   41.036,   43.047],\n",
       "        [  57.74 ,   54.028,   64.217, ...,   28.16 ,   51.281,   51.978],\n",
       "        [  51.751,   60.256,   59.619, ...,   23.16 ,   35.096,   60.907],\n",
       "        ..., \n",
       "        [  18.05 ,   14.039,   14.811, ...,   48.72 ,   89.805,   81.271],\n",
       "        [  26.039,   14.153,   18.153, ...,   59.948,  209.805,  132.869],\n",
       "        [  32.05 ,   16.153,   14.039, ...,   74.72 ,  193.805,  196.869]],\n",
       "\n",
       "       ..., \n",
       "       [[  69.751,   73.751,   73.74 , ...,   58.716,   67.011,   62.979],\n",
       "        [  70.751,   76.979,   70.196, ...,   60.954,   66.064,   63.299],\n",
       "        [  74.028,   85.185,   87.619, ...,   54.16 ,   51.096,   60.075],\n",
       "        ..., \n",
       "        [  69.997,   67.182,   67.965, ...,  114.847,   60.716,   91.075],\n",
       "        [  55.182,   71.193,   41.036, ...,   53.619,   63.075,   86.716],\n",
       "        [  35.16 ,   75.014,   58.036, ...,  108.836,   87.135,   73.907]],\n",
       "\n",
       "       [[  77.185,   82.619,   57.726, ...,   64.185,   78.897,   64.185],\n",
       "        [  80.168,   82.819,   18.665, ...,   69.701,   77.929,   65.897],\n",
       "        [  79.157,   74.965,   19.893, ...,   71.011,   78.299,   58.897],\n",
       "        ..., \n",
       "        [  18.   ,   20.31 ,   15.168, ...,   21.   ,   21.   ,   26.   ],\n",
       "        [  21.31 ,   22.94 ,   17.228, ...,   25.712,   15.712,   26.869],\n",
       "        [  18.94 ,   20.168,   23.869, ...,   15.712,   16.712,   24.228]],\n",
       "\n",
       "       [[  12.816,   16.153,   14.854, ...,   38.978,   42.946,   42.256],\n",
       "        [  14.039,   14.327,   15.082, ...,   17.082,   14.039,   15.153],\n",
       "        [  15.05 ,   15.327,   14.082, ...,   18.794,   16.039,   13.039],\n",
       "        ..., \n",
       "        [  39.973,   44.261,   84.164, ...,   62.186,  159.08 ,  155.992],\n",
       "        [ 111.022,  173.891,  103.072, ...,  164.902,  159.351,  162.736],\n",
       "        [ 193.046,  185.981,  210.211, ...,  164.992,  165.192,  164.736]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
